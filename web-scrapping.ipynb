{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90d85ba",
   "metadata": {},
   "source": [
    "# Web-scrapping com Python\n",
    "\n",
    "> DISCLAIMER: Notebook com fins educacionais.\n",
    "\n",
    "## Obtendo tabelas online\n",
    "\n",
    "Esse script mostra como obter dados do [Ranking 1500 - Empresas Mais](https://publicacoes.estadao.com.br/empresasmais/ranking-1500/) utilizando Python. Foram utilizadas 5 bibliotecas, sendo uma opcional:\n",
    "\n",
    "* [BeastifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - manipula informações em HTML e XML;\n",
    "* [requests](https://requests.readthedocs.io/en/latest/) - biblioteca HTTP para python;\n",
    "* [Numpy](https://numpy.org/doc/stable) - manipulação de dados;\n",
    "* [Pandas](https://pandas.pydata.org/docs/) - manipulação de tabelas;\n",
    "* time (opcional) foi utilizada apenas para marcar tempo entre iterações. \n",
    "\n",
    "### Instalação\n",
    "\n",
    "Para garantir que as bibliotecas estejam instaladas é possível chamar `\"pip install <nome-biblioteca>\"` no prompt de comando. Nos notebooks fazemos essa chamada precidada por pontos de exclamação para sinalizar que esse trecho deve ser executado no prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cec02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291d7dc",
   "metadata": {},
   "source": [
    "Para aqueles que usando o Conda ou Miniconda é possível fazer a mesma chamada utilizando `conda` no lugar de `pip`.\n",
    "\n",
    "## Obtendo os dados\n",
    "\n",
    "A palavra reservada `import` carrega as bibliotecas. É possível carregar módulos e outros objetos separados com a palavra reservada `from` como podemos ver na próxima linha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa842585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time # opcional para imprimir tempo de execução no final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6574b4",
   "metadata": {},
   "source": [
    "Na primeira linha temos `from bs4 import BeautifulSoup` lê-se \"Da **biblioteca bs4** importe o **módulo BeautifulSoup**\".As vezes o nome utilizado para instalação difere do nome utilizado para importa-lo. Também podemos dar \"apelidos\" para as bibliotecas que carregamos utilizando a palavra reservada `as`. Chamar pandas de pd e numpy de np é uma prática muito comum entre pythonistas.\n",
    "\n",
    "Obter tabelas de alguns sites é tão simples quanto chamar `pd.read_html(<site>)` (tente isso com qualquer página da Wikipedia contendo uma tabela e veja o que acontece). Outros sites dificultam a obtenção de dados de forma automatica (robôs). Nestes casos utilizamos a biblioteca requests para simular o que seria uma requisição de um navegador comum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5019d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://publicacoes.estadao.com.br/empresasmais/ranking-1500/'\n",
    "\n",
    "header = {\n",
    "  'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36',\n",
    "  'X-Requested-With': 'XMLHttpRequest'\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=header)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f28759",
   "metadata": {},
   "source": [
    "Dados da página foram obtidos por meio de uma requisição HTTP. A resposta 200 mostra que nosso chamado deu certo. O passo seguinte é interpretar esses dados (\"parsar\") com bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6632fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37b78f",
   "metadata": {},
   "source": [
    "Descomentando a última linha acima vemos o código HTML completo da página. Ela utiliza tags, atributos e ids para organizar o conteúdo da página. Utilizando a ferramenta de desenvolvedor do seu navegador você conseguirá encontrar elementos/padrões utilizados para organizar a tabela. Neste caso um elemento que embrulhava toda a tablea era a classe \"ranking-table\". Podemos analisar os elementos dele usando o método `find` do bs4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d5f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='ranking-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc800869",
   "metadata": {},
   "source": [
    "> Curiosidade: a palavra \"class\" é reservada no Python e utilizada para Programação Orientada a Objetos (do inglês OOP), por isso o find ussa class_ como nome do argumento.\n",
    "\n",
    "Os elementos que precisamos estão debaixo de classes mas nem sempre é assim. Eles podem estar ligados a ids ou outros atributos. Para fins didaticos começaremos ignorando o fato que são várias páginas e não uma única página. Depois de criar a lógica para obter os dados da primeira página, obteremos os resultados da demais aplicando um *loop*.\n",
    "\n",
    "O primeiro passo é identificar os elementos ligados ao cabeçalho e dados. Se abrir o site irá notar que cada linha da tabela contém outra tabela. Explorando os padrões contidos em `ranking_table` descobri o seguinte:\n",
    "\n",
    "* A **classe** \"ranking-table__th\" contém o cabeçalho maior;\n",
    "* A **classe** \"ranking-table__td\" tem os dados relacionados a esse cabeçalho;\n",
    "* As **classes** \"ranking-table__sub-item-title\" e \"ranking-table__sub-item-value\" contam respectivavemente com o cabeçalho e dados da tabela menor.\n",
    "\n",
    "A seguir eu uso o primeiro padrão dentro do que chamamos de [*list comprehension*](https://www.w3schools.com/python/python_lists_comprehension.asp) para obter o primeiro cabeçalho. Também utizei o *slicing* para ignorar o primeiro e último caractéres que era um escape (\\n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8554d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_header = [i.text[1:-1] for i in soup.find_all(class_='ranking-table__th')] # isso é uma list comprehension\n",
    "main_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18eb008",
   "metadata": {},
   "source": [
    "Na sequência usei a mesma lógica, transformei o resultado usando *Numpy* para trocar uma uníca e longa lista em várias listas de de 5 elementos, imitando uma matriz de 5 colunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34c0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = [i.text[1:-1] for i in soup.find_all(class_='ranking-table__td')] # separando os dados\n",
    "main_data = np.array(main_data) # transformando em um array numpy\n",
    "main_data = main_data.reshape((-1,5)) # trocando o formato para ter 5 colunas\n",
    "main_data[:5] # exibindo as 5 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb013b",
   "metadata": {},
   "source": [
    "Já temos os dados da primeira tabela. Usando *Pandas* conseguimos convertê-la no formato esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d005aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame(main_data, columns=main_header)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838c62a",
   "metadata": {},
   "source": [
    "*Pandas* cuida de exprimir os resultados em um formato amigável. Também torna fácil exportar para formatos como *Excel* e *CSV* (mais disso no final). Agora precisamos dos dados e cabeçalhos da segunda tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1349a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_header = [i.text for i in soup.find_all(class_='ranking-table__sub-item-title')]\n",
    "sub_header = sub_header[: len(set(sub_header))]\n",
    "\n",
    "sub_data = [i.text for i in soup.find_all(class_='ranking-table__sub-item-value')]\n",
    "sub_data = np.array(sub_data)\n",
    "sub_data = sub_data.reshape(-1, len(sub_header))\n",
    "\n",
    "\n",
    "df_2 = pd.DataFrame(sub_data, columns=sub_header)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47872465",
   "metadata": {},
   "source": [
    "Aqui utilizei dois novos truques:\n",
    "\n",
    "1. A função `set` converte um objeto em conjunto, eliminando elementos repetidos - os cabeçalhos da tabela menor se repetiam diversas vezes;\n",
    "2. A função `len` mostra o número de elementos em um conjunto ou lista (do inglês *lenght*);\n",
    "\n",
    "Agora que temos a primeira tabela com 100 linhas e 5 colunas e a segunda tabela com 100 linhas e 15 colunas podemos juntar ambas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad910930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca15e0",
   "metadata": {},
   "source": [
    "O último passo seria exportar para um excel usando `df.to_excel('ranking_1500.xlsx')`ou para um csv com `df.to_csv('ranking_1500.csv', index=False)` mas ainda iremos iterar em cada página do site para completar a tabela. Sabendo que são 1500 empresas e 100 empresas por páginas é fácil deduzir que serão 15 páginas. Mas e se isso mudar? Existe uma forma \"automática\" de obter esse dado? A resposta é sim.\n",
    "\n",
    "Observando novamente o padrão da página encontre o hiperlink que aponta para última página na classe \"last\". Para obter esse dado é simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40044891",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='last')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ba6de",
   "metadata": {},
   "source": [
    "No próximo passo eu embrulho tudo isso em um loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d510dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = soup.find(class_='last')['href'] # obtendo o link\n",
    "last_page = last_page.split('/')[-2] # separando os elementos entre / e acessando o penúltimo\n",
    "\n",
    "print(f'A última página é a {last_page}') # imprimindo o resultado\n",
    "\n",
    "last_page = int(last_page) # convertendo em inteiro\n",
    "\n",
    "df_final = pd.DataFrame() # iniciando uma tabela vazia para guardar cada página\n",
    "\n",
    "for page in range(last_page):\n",
    "    url = f'https://publicacoes.estadao.com.br/empresasmais/ranking-1500/{page+1:0.0f}'\n",
    "    time_now = time.strftime('%H:%M:%S')\n",
    "    print(f'[{time_now}] Obtendo resultados da página {url}')\n",
    "    \n",
    "    # repetindo os passos de cima para cada página\n",
    "    r = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    \n",
    "    main_header = [i.text[1:-1] for i in soup.find_all(class_='ranking-table__th')] \n",
    "\n",
    "    main_data = [i.text[1:-1] for i in soup.find_all(class_='ranking-table__td')] \n",
    "    main_data = np.array(main_data) \n",
    "    main_data = main_data.reshape((-1,5))\n",
    "\n",
    "    df_1 = pd.DataFrame(main_data, columns=main_header)\n",
    "\n",
    "    sub_header = [i.text for i in soup.find_all(class_='ranking-table__sub-item-title')]\n",
    "    sub_header = sub_header[: len(set(sub_header))]\n",
    "\n",
    "    sub_data = [i.text for i in soup.find_all(class_='ranking-table__sub-item-value')]\n",
    "    sub_data = np.array(sub_data)\n",
    "    sub_data = sub_data.reshape(-1, len(sub_header))\n",
    "\n",
    "    df_2 = pd.DataFrame(sub_data, columns=sub_header)\n",
    "    \n",
    "    df = pd.concat([df_1, df_2], axis=1)\n",
    "\n",
    "    df_final = pd.concat([df_final, df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e938e7",
   "metadata": {},
   "source": [
    "Dando uma espiada no resultado final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bc56002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e156b",
   "metadata": {},
   "source": [
    "Guardando o resultado em um arquivo excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202f14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_excel('ranking_1500.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
